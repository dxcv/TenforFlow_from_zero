# æ¥æº
bilibiliè§†é¢‘:  
[ã€tensorflowã€‘åŸºç¡€æ•™å­¦è¯¾ç¨‹](https://www.bilibili.com/video/av25541757)   

icourse163ï¼š   
[äººå·¥æ™ºèƒ½å®è·µï¼šTensorflowç¬”è®°](https://www.icourse163.org/course/PKU-1002536002)    



# 3.1 å¼ é‡ è®¡ç®—å›¾ ä¼šè¯
å¼ é‡ï¼šå¤šç»´æ•°ç»„ï¼ˆåˆ—è¡¨ï¼‰
é˜¶ï¼š å¼ é‡çš„ç»´æ•°
æ•°æ®ç±»å‹ï¼štf.float32 tf.int32 ...

# 3.2 å‰å‘ä¼ æ’­
## å‚æ•°ï¼š çº¿ä¸Šçš„æƒé‡
```
w1=tf.Variable(tf.random_normal([2,3],stddev=2,mean=0,seed=1))
w2=tf.Variable(tf.truncated_normal([2,3],stddev=2,mean=0,seed=1))
w3=tf.Variable(tf.random_uniform([2,3],minval=0,maxval=1))
```

w1,æ­£æ€åˆ†å¸ƒï¼Œ2x3çŸ©é˜µ æ ‡å‡†å·®ä¸º2 å‡å€¼ä¸º0 éšæœºç§å­ä¸º1  
w2,å»æ‰è¿‡å¤§åç¦»ç‚¹çš„æ­£æ€åˆ†å¸ƒ  
w3,åœ¨å‡åŒ€åˆ†å¸ƒä¸­éšæœºé‡‡æ ·

# 3.3 åå‘ä¼ æ’­
åå‘ä¼ æ’­ï¼šä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œåœ¨æ‰€æœ‰å‚æ•°ä¸Šç”¨æ¢¯åº¦ä¸‹é™ï¼Œä½¿NNæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æŸå¤±å‡½æ•°æœ€å°ã€‚  

lossï¼š é¢„æµ‹å€¼ä¸å·²çŸ¥ç­”æ¡ˆçš„å·®è·  

å‡æ–¹è¯¯å·®MSEï¼šy_å’Œyçš„å·®çš„å¹³æ–¹çš„å¹³å‡æ•°
`loss=tf.reduce_mean(tf.square(y_-y))`

åå‘ä¼ æ’­è®­ç»ƒæ–¹æ³•
```
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)
train_step = tf.train.MomentumOptimizer(learning_rate,momentum).minimize(loss)
```

å­¦ä¹ ç‡ï¼šå†³å®šå‚æ•°æ¯æ¬¡æ›´æ–°çš„å¹…åº¦


# 4.1 æ¿€æ´»å‡½æ•° æŸå¤±å‡½æ•°

```
relu      fx=max(x,0)
sigmoid   fx=1/(1+e-x)
tanh      fx=(1-e-2x)/(1+e-2x)

loss = tf.reduce_mean(tf.square(y_-y))
loss = tf.reduce_sum(tf.where(tf.greater(y,y_),COST*(y-y_),PROFIT*(y_-y)))
```


eq.é”€é‡ä¸x1x2æœ‰å…³ y_=x1+x2 å™ªå£°-0.05~+0.05

## äº¤å‰ç†µ
H(y_.y) = - âˆ‘ y_ * logy
ce = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-12,1.0)))
ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))
cem=tf.reduce_mean(ce)

# 4.2 å­¦ä¹ ç‡
æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡
```

LEARNING_RATE_BASE        = 0.1
LEARNING_RATE_DECAY_STEPS = 1
LEARNING_RATE_DECAY_RATE  = 0.99
global_step = tf.Variable(0,trainable=False)
learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_DECAY_STEPS,LEARNING_RATE_DECAY_RATE,staircase=True)
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)
```

# 4.3 æ»‘åŠ¨å¹³å‡

å½±å­     = è¡°å‡ç‡*å½±å­ + ï¼ˆ1-è¡°å‡ç‡)*å‚æ•°
å½±å­åˆå€¼ = å‚æ•°åˆå€¼
è¡°å‡ç‡   = min{MOVING_AVERAGE_DECAY, 1+è½®æ•°/10+è½®æ•°}
```
ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)
```

ema.apply()å‡½æ•°å®ç°å¯¹æ‹¬å·å†…å‚æ•°æ±‚æ»‘åŠ¨å¹³å‡ï¼Œtf.trainable_variables()å‡½æ•°å®ç°æŠŠæ‰€æœ‰
å¾…è®­ç»ƒå‚æ•°æ±‡æ€»ä¸ºåˆ—è¡¨ã€‚
```
ema_op = ema.apply(tf.trainable_variables()) 
```


tf.control_dependencieså‡½æ•°å®ç°å°†æ»‘åŠ¨å¹³å‡å’Œè®­ç»ƒè¿‡ç¨‹åŒæ­¥è¿è¡Œã€‚
```
with tf.control_dependencies([train_step, ema_op]):
    train_op = tf.no_op(name='train') 
```

æŸ¥çœ‹æ¨¡å‹ä¸­å‚æ•°çš„å¹³å‡å€¼ï¼Œå¯ä»¥ç”¨ ema.average()å‡½æ•°ã€‚
```
sess.run(ema.average(w1)
```

# è¿‡æ‹Ÿåˆ æ­£åˆ™åŒ–
âˆšè¿‡æ‹Ÿåˆï¼šç¥ç»ç½‘ç»œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾ƒé«˜ï¼Œåœ¨æ–°çš„æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–åˆ†ç±»æ—¶å‡†ç¡®ç‡è¾ƒ
ä½ï¼Œè¯´æ˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å·®ã€‚
âˆšæ­£åˆ™åŒ–ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­ç»™æ¯ä¸ªå‚æ•° w åŠ ä¸Šæƒé‡ï¼Œå¼•å…¥æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡ï¼Œä»è€ŒæŠ‘åˆ¶æ¨¡å‹å™ªå£°ï¼Œå‡å°
è¿‡æ‹Ÿåˆã€‚ 


ä½¿ç”¨æ­£åˆ™åŒ–åï¼ŒæŸå¤±å‡½æ•° loss å˜ä¸ºä¸¤é¡¹ä¹‹å’Œï¼š
```
loss = loss_orign + REGULARIZER*loss(w)
```

loss(w)æœ‰ä¸¤ç§ l1ä¸l2
â‘  L1 æ­£åˆ™åŒ–ï¼š ğ’ğ’ğ’”ğ’”ğ‘³ğŸ = âˆ‘ğ’Š |ğ’˜ğ’Š |
`loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w)`
â‘¡ L2 æ­£åˆ™åŒ–ï¼š ğ’ğ’ğ’”ğ’”ğ‘³ğŸ = âˆ‘ğ’Š |ğ’˜ğ’Š | ğŸ 
`loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w)`


## eq éšæœºç‚¹ï¼Œ  x0^2+x1^2<2æ—¶ ä¸º1ç±» å…¶ä»–2ç±»
```
import matplotlib.pyplot as plt
plt.scatter(x,y,c="é¢œè‰²")
plt.show()
```

xx,yy=np.mgrid[-5:5:0.25,-5:5:0.25]
grid = np.c_[xx.ravel(),yy.ravel()]




# åŸºæœ¬æ“ä½œ
## æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½

### ä¿å­˜
åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œä¸€èˆ¬ä¼šé—´éš”ä¸€å®šè½®æ•°ä¿å­˜ä¸€æ¬¡ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¹¶äº§ç”Ÿä¸‰ä¸ªæ–‡ä»¶ï¼ˆä¿å­˜å½“å‰å›¾ç»“æ„çš„.meta æ–‡ä»¶ã€ä¿å­˜å½“å‰å‚æ•°åçš„.index æ–‡ä»¶ã€ä¿å­˜å½“å‰å‚æ•°çš„.data æ–‡ä»¶ï¼‰ï¼Œåœ¨ Tensorflow ä¸­å¦‚ä¸‹è¡¨ç¤ºï¼š
```
saver = tf.train.Saver()          
with tf.Session() as sess:    
    for i in range(STEPS): 
        if i % è½®æ•° == 0:          
            saver.save(sess, os.path.join(MODEL_SAVE_PATH,MODEL_NAME), global_step=global_step)
```
### åŠ è½½

è‹¥ ckpt å’Œä¿å­˜çš„æ¨¡å‹åœ¨æŒ‡å®šè·¯å¾„ä¸­å­˜åœ¨ï¼Œåˆ™å°†ä¿å­˜çš„ç¥ç»ç½‘ç»œæ¨¡å‹åŠ è½½åˆ°å½“å‰ä¼šè¯ä¸­ã€‚
```
with tf.Session() as sess: 
    ckpt = tf.train.get_checkpoint_state(å­˜å‚¨è·¯å¾„) 
    if ckpt and ckpt.model_checkpoint_path: 
      saver.restore(sess, ckpt.model_checkpoint_path) 
```

### åŠ è½½æ»‘åŠ¨å¹³å‡
åœ¨ä¿å­˜æ¨¡å‹æ—¶ï¼Œè‹¥æ¨¡å‹ä¸­é‡‡ç”¨æ»‘åŠ¨å¹³å‡ï¼Œåˆ™å‚æ•°çš„æ»‘åŠ¨å¹³å‡å€¼ä¼šä¿å­˜åœ¨ç›¸åº”æ–‡ä»¶ä¸­ã€‚é€šè¿‡å®ä¾‹åŒ– saver å¯¹è±¡ï¼Œå®ç°å‚æ•°æ»‘åŠ¨å¹³å‡å€¼çš„åŠ è½½ï¼Œåœ¨ Tensorflow ä¸­å¦‚ä¸‹è¡¨ç¤º
```
ema = tf.train.ExponentialMovingAverage(æ»‘åŠ¨å¹³å‡åŸºæ•°) 
ema_restore = ema.variables_to_restore()         
saver = tf.train.Saver(ema_restore) 
```

# mnist ç¥ç»ç½‘ç»œæ¨¡å‹å‡†ç¡®ç‡è¯„ä¼°æ–¹æ³• 
åœ¨ç½‘ç»œè¯„ä¼°æ—¶ï¼Œä¸€èˆ¬é€šè¿‡è®¡ç®—åœ¨ä¸€ç»„æ•°æ®ä¸Šçš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œè¯„ä¼°ç¥ç»ç½‘ç»œçš„æ•ˆæœã€‚
```
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) 
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 
```




# ä¸€äº›å‡½æ•°

tf.get_collection("")             | ä» collection é›†åˆä¸­å–å‡ºå…¨éƒ¨å˜é‡ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨
tf.add()                          | å°†å‚æ•°åˆ—è¡¨ä¸­å¯¹åº”å…ƒç´ ç›¸åŠ 
tf.convert_to_tensor              | è½¬æ¢ä¸ºtfæ•°æ®æ ¼å¼
tf.cast(x,dtype)                  | å°†å‚æ•° x è½¬æ¢ä¸ºæŒ‡å®šæ•°æ®ç±»å‹
tf.equal()                        | å¯¹æ¯”ä¸¤ä¸ªçŸ©é˜µæˆ–è€…å‘é‡çš„å…ƒç´ ã€‚è‹¥å¯¹åº”å…ƒç´ ç›¸ç­‰ï¼Œåˆ™è¿” å› Trueï¼›è‹¥å¯¹åº”å…ƒç´ ä¸ç›¸ç­‰ï¼Œåˆ™è¿”å› Falseã€‚
tf.reduce_mean(x,axis)            | æ±‚å–çŸ©é˜µæˆ–å¼ é‡æŒ‡å®šç»´åº¦çš„å¹³å‡å€¼ã€‚è‹¥ä¸æŒ‡å®šç¬¬äºŒä¸ªå‚æ•°ï¼Œåˆ™åœ¨æ‰€æœ‰å…ƒç´ ä¸­å–å¹³å‡å€¼ï¼›è‹¥æŒ‡å®šç¬¬äºŒä¸ªå‚æ•°ä¸º 0ï¼Œåˆ™åœ¨ç¬¬ä¸€ç»´å…ƒç´ ä¸Šå–å¹³å‡å€¼ï¼Œå³æ¯ä¸€åˆ—æ±‚å¹³å‡å€¼ï¼›è‹¥æŒ‡å®šç¬¬äºŒä¸ªå‚æ•°ä¸º 1ï¼Œåˆ™åœ¨ç¬¬äºŒç»´å…ƒç´ ä¸Šå–å¹³å‡å€¼ï¼Œå³æ¯ä¸€è¡Œæ±‚å¹³å‡å€¼ã€‚
tf.argmax(x,axis)                 | è¿”å›æŒ‡å®šç»´åº¦ axis ä¸‹ï¼Œå‚æ•° x ä¸­æœ€å¤§å€¼ç´¢å¼•å·ã€‚
with tf.Graph().as_default() as g | å°†å½“å‰å›¾è®¾ç½®æˆä¸ºé»˜è®¤å›¾ï¼Œå¹¶è¿”å›ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚åº”ç”¨äºå°†å·²ç»å®šä¹‰å¥½çš„ç¥ç»ç½‘ç»œåœ¨è®¡ç®—å›¾ä¸­å¤ç°ã€‚



os.path.join('/hello/','good/boy/','doiido')             | æ‹¼æ¥è·¯å¾„
'./model/mnist_model-1001'.split('/')[-1].split('-')[-1] |æ‹†åˆ†è·¯å¾„


```
A = tf.convert_to_tensor(np.array([[1,1,2,4], [3,4,8,5]]))   
print A.dtype   
b = tf.cast(A, tf.float32)   
print b.dtype 
```
è¾“å‡ºç»“æœï¼š  
<dtype: 'int64'> 
<dtype: 'float32'> 

## tf.equal()
```
A = [[1,3,4,5,6]]   
B = [[1,3,4,3,2]]   
with tf.Session() as sess:   
    print(sess.run(tf.equal(A, B))) 
```
è¾“å‡ºç»“æœï¼š[[ True  True  True False False]]


## tf.reduce_mean(x,axis)
```
x = [[1., 1.] [2., 2.]] 
print(tf.reduce_mean(x)) 
print(tf.reduce_mean(x, 0)) 
print(tf.reduce_mean(x, 1)) 
```
è¾“å‡ºç»“æœï¼š
1.5    
[1.5, 1.5]    
[1., 1.]    

## tf.argmax(x,axis)
tf.argmax([1,0,0],1)
å‡½æ•°ä¸­ï¼Œaxis ä¸º 1ï¼Œå‚æ•° x ä¸º[1,0,0]ï¼Œè¡¨ç¤ºåœ¨å‚æ•° x
çš„ç¬¬ä¸€ä¸ªç»´åº¦å–æœ€å¤§å€¼å¯¹åº”çš„ç´¢å¼•å·ï¼Œæ•…è¿”å› 0ã€‚ 