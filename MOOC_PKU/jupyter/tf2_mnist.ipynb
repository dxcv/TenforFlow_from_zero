{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    BATCH_SIZE = 30\n",
    "    STEPS      = 40001\n",
    "\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets('./mnist_data/',one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "1\t After     1 steps, loss on train batch  is \t 2.983377\n",
      "2\t                  loss on test  batch  is \t \t 2.544055\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.397500\n",
      "1\t After   500 steps, loss on train batch  is \t 0.951247\n",
      "2\t                  loss on test  batch  is \t \t 0.992908\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.935200\n",
      "1\t After  1000 steps, loss on train batch  is \t 0.987219\n",
      "2\t                  loss on test  batch  is \t \t 0.936495\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.947800\n",
      "1\t After  1500 steps, loss on train batch  is \t 0.869553\n",
      "2\t                  loss on test  batch  is \t \t 0.904652\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.954900\n",
      "1\t After  2000 steps, loss on train batch  is \t 0.828865\n",
      "2\t                  loss on test  batch  is \t \t 0.878943\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.959000\n",
      "1\t After  2500 steps, loss on train batch  is \t 0.846661\n",
      "2\t                  loss on test  batch  is \t \t 0.861288\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.963200\n",
      "1\t After  3000 steps, loss on train batch  is \t 0.851659\n",
      "2\t                  loss on test  batch  is \t \t 0.842735\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.966200\n",
      "1\t After  3500 steps, loss on train batch  is \t 0.833306\n",
      "2\t                  loss on test  batch  is \t \t 0.829324\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.967500\n",
      "1\t After  4000 steps, loss on train batch  is \t 0.794569\n",
      "2\t                  loss on test  batch  is \t \t 0.817399\n",
      "3\t                  acc  on test  batch  is \t \t \t 0.970500\n"
     ]
    }
   ],
   "source": [
    "#coding:utf=8\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#定义神经网络的输入、参数和输出，定义前向传播过程\n",
    "def get_weight(shape,regularizer):\n",
    "    w = tf.Variable(tf.random_normal(shape),dtype=tf.float32)\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "\n",
    "def forward(x,regularizer):\n",
    "\n",
    "    w1 = get_weight([INPUT_NODE,LAYER1_NODE],regularizer)\n",
    "    b1 = get_bias([LAYER1_NODE])\n",
    "\n",
    "    w2 = get_weight([LAYER1_NODE,OUTPUT_NODE],regularizer)\n",
    "    b2 = get_bias([OUTPUT_NODE])\n",
    "\n",
    "\n",
    "    y1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    y2 = tf.matmul(y1,w2)+b2\n",
    "\n",
    "    Yo = y2\n",
    "\n",
    "    return Yo\n",
    "\n",
    "def backward(mnist):\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    Xi = tf.placeholder(tf.float32,shape=(None,INPUT_NODE))\n",
    "    Yi = tf.placeholder(tf.float32,shape=(None,OUTPUT_NODE))\n",
    "    Yo = forward(Xi,REGULARIZER)\n",
    "\n",
    "\n",
    "    # loss_mse = tf.reduce_mean(tf.square(Yo-Yi))\n",
    "    loss_mse = tf.reduce_mean(tf.square(tf.cast(tf.equal(tf.argmax(Yo, 1), tf.argmax(Yi, 1)),tf.float32)-1.0))\n",
    "    loss_cem = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=Yo,labels=tf.argmax(Yi,1)))\n",
    "    lossA = loss_mse\n",
    "    lossB = loss_cem\n",
    "    lossC = loss_mse + tf.add_n(tf.get_collection('losses'))\n",
    "    lossD = loss_cem + tf.add_n(tf.get_collection('losses'))\n",
    "    loss  = lossD\n",
    "\n",
    "\n",
    "\n",
    "    learning_rate_exponential_decay = tf.train.exponential_decay(\n",
    "                                        LEARNING_RATE_BASE,\n",
    "                                        global_step,\n",
    "                                        mnist.train.num_examples/BATCH_SIZE,\n",
    "                                        LEARNING_RATE_DECAY_RATE,\n",
    "                                        staircase=True)\n",
    "    learning_rateA = learning_rate_exponential_decay\n",
    "    learning_rateB = 0.01\n",
    "    learning_rate  = learning_rateA\n",
    "\n",
    "\n",
    "\n",
    "    train_step_GradientDescentOptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    MOMENTUM=0.9\n",
    "    train_step_MomentumOptimizer = tf.train.MomentumOptimizer(learning_rate,MOMENTUM).minimize(loss,global_step=global_step)\n",
    "    train_step_AdamOptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "    train_stepA = train_step_GradientDescentOptimizer\n",
    "    train_stepB = train_step_MomentumOptimizer\n",
    "    train_stepC = train_step_AdamOptimizer\n",
    "\n",
    "    train_step  = train_stepA\n",
    "\n",
    "\n",
    "    #滑动平均\n",
    "\n",
    "    if USE_EMA__ :\n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "        ema_op = ema.apply(tf.trainable_variables())\n",
    "        with tf.control_dependencies([train_step,ema_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "    else:\n",
    "        train_op = train_step\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #实例化saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #测试准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Yo, 1), tf.argmax(Yi, 1)),tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        #初始化\n",
    "        init=tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # #断点续训\n",
    "        # ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        # if ckpt and ckpt.model_checkpoint_path:\n",
    "        #     saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "\n",
    "        Xt,Yt = (mnist.test.images,mnist.test.labels)\n",
    "        for i in range(STEPS):\n",
    "            #训练模型\n",
    "            Xs,Ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={Xi:Xs,Yi:Ys})\n",
    "\n",
    "            if  (i > 0  and  i % 500 == 0) or (i == 1) :\n",
    "                #插入测试\n",
    "                loss_v = sess.run(loss,feed_dict={Xi:Xt,Yi:Yt})\n",
    "                acc_v  = sess.run(accuracy,feed_dict={Xi:Xt,Yi:Yt})\n",
    "                print(\"1\\t After %5d steps, loss on train batch  is \\t %f\"%(i,loss_value))\n",
    "                print(\"2\\t                  loss on test  batch  is \\t \\t %f\"%(loss_v))\n",
    "                print(\"3\\t                  acc  on test  batch  is \\t \\t \\t %f\"%(acc_v))\n",
    "\n",
    "            if i % 2000 == 0 :\n",
    "                #保存模型\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_SAVE_NAME),global_step=global_step)\n",
    "\n",
    "\n",
    "def test(mnist_test):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        Xt= tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "        Yt= tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "        Yo = forward(Xt,REGULARIZER)\n",
    "\n",
    "        if USE_EMA__ :\n",
    "            ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "            ema_restore = ema.variables_to_restore()\n",
    "            saver = tf.train.Saver(ema_restore)\n",
    "        else:\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Yo, 1), tf.argmax(Yt, 1)),tf.float32))\n",
    "\n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/'[-1]).split('-')[-1]\n",
    "                    acc=sess.run(accuracy,feed_dict={Xt:mnist_test.test.images,Yt:mnist_test.test.labels})\n",
    "                    print(\"1\\t After %5d steps, test acc is \\t %f\"%(global_step,acc))\n",
    "                else:\n",
    "                    print('No ckpt file found')\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    mnist = input_data.read_data_sets('./mnist_data/',one_hot=True)\n",
    "\n",
    "    # # 返回 训练 验证 测试集 子集样本数\n",
    "    # print(\"train data size:\",mnist.train.num_examples)\n",
    "    # print(\"validation data size:\",mnist.validation.num_examples)\n",
    "    # print(\"test data size:\",mnist.test.num_examples)\n",
    "\n",
    "    # #返回数据和标签\n",
    "    # print(mnist.train.images[0])\n",
    "    # print(mnist.train.labels[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    USE_EMA__ = False\n",
    "\n",
    "    INPUT_NODE  = 784\n",
    "    OUTPUT_NODE = 10\n",
    "    LAYER1_NODE = 500\n",
    "\n",
    "    BATCH_SIZE  = 200\n",
    "    STEPS       = 4500\n",
    "\n",
    "    REGULARIZER               = 0.0001\n",
    "    LEARNING_RATE_BASE        = 0.1\n",
    "    LEARNING_RATE_DECAY_STEPS = 1000\n",
    "    LEARNING_RATE_DECAY_RATE  = 0.99\n",
    "    MOVING_AVERAGE_DECAY      = 0.99\n",
    "\n",
    "    MODEL_SAVE_PATH = \"./model/\"\n",
    "    MODEL_SAVE_NAME = \"mnist_model\"\n",
    "\n",
    "    backward(mnist)\n",
    "\n",
    "\n",
    "    # TEST_INTERVAL_SECS = 5\n",
    "    # mnist_test = input_data.read_data_sets('./mnist_data/',one_hot=True)\n",
    "    # test(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "application\n",
      "pre_pic\n",
      "forward\n",
      "get_weight\n",
      "get_bias\n",
      "get_weight\n",
      "get_bias\n",
      "INFO:tensorflow:Restoring parameters from ./model/mnist_model-4001\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [784,500] rhs shape= []\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable, save/RestoreV2/_11)]]\n\t [[Node: save/RestoreV2_1/_8 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_21_save/RestoreV2_1\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/RestoreV2_1)]]\n\nCaused by op 'save/Assign', defined at:\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-622a8f34d1af>\", line 271, in <module>\n    application()\n  File \"<ipython-input-4-622a8f34d1af>\", line 197, in application\n    preValue = restore_model(testPicArr)\n  File \"<ipython-input-4-622a8f34d1af>\", line 214, in restore_model\n    saver = tf.train.Saver()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1056, in __init__\n    self.build()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 270, in assign\n    validate_shape=validate_shape)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [784,500] rhs shape= []\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable, save/RestoreV2/_11)]]\n\t [[Node: save/RestoreV2_1/_8 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_21_save/RestoreV2_1\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/RestoreV2_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [784,500] rhs shape= []\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable, save/RestoreV2/_11)]]\n\t [[Node: save/RestoreV2_1/_8 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_21_save/RestoreV2_1\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/RestoreV2_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-622a8f34d1af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAIN_STEP\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mapplication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-622a8f34d1af>\u001b[0m in \u001b[0;36mapplication\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0mtestPic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./test/5.png'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mtestPicArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_pic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestPic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mpreValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestPicArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The prediction number is:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-622a8f34d1af>\u001b[0m in \u001b[0;36mrestore_model\u001b[1;34m(testPicArr)\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0mckpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m                 \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m                 \u001b[0mpreValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreValue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtestPicArr\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpreValue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1457\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [784,500] rhs shape= []\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable, save/RestoreV2/_11)]]\n\t [[Node: save/RestoreV2_1/_8 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_21_save/RestoreV2_1\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/RestoreV2_1)]]\n\nCaused by op 'save/Assign', defined at:\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-622a8f34d1af>\", line 271, in <module>\n    application()\n  File \"<ipython-input-4-622a8f34d1af>\", line 197, in application\n    preValue = restore_model(testPicArr)\n  File \"<ipython-input-4-622a8f34d1af>\", line 214, in restore_model\n    saver = tf.train.Saver()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1056, in __init__\n    self.build()\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 270, in assign\n    validate_shape=validate_shape)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\0_Yeran\\0_tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [784,500] rhs shape= []\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable, save/RestoreV2/_11)]]\n\t [[Node: save/RestoreV2_1/_8 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_21_save/RestoreV2_1\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](save/RestoreV2_1)]]\n"
     ]
    }
   ],
   "source": [
    "#coding:utf=8\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#定义神经网络的输入、参数和输出，定义前向传播过程\n",
    "def get_weight(shape,regularizer):\n",
    "    print(\"get_weight\")\n",
    "    w = tf.Variable(tf.random_normal(shape),dtype=tf.float32)\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):\n",
    "    print(\"get_bias\")\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "\n",
    "def forward(x,regularizer):\n",
    "    print(\"forward\")\n",
    "\n",
    "    w1 = get_weight([INPUT_NODE,LAYER1_NODE],regularizer)\n",
    "    b1 = get_bias([LAYER1_NODE])\n",
    "\n",
    "    w2 = get_weight([LAYER1_NODE,OUTPUT_NODE],regularizer)\n",
    "    b2 = get_bias([OUTPUT_NODE])\n",
    "\n",
    "\n",
    "    y1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    y2 = tf.matmul(y1,w2)+b2\n",
    "\n",
    "    Yo = y2\n",
    "\n",
    "    return Yo\n",
    "\n",
    "def backward(mnist):\n",
    "    print(\"backward\")\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    Xi = tf.placeholder(tf.float32,shape=(None,INPUT_NODE))\n",
    "    Yi = tf.placeholder(tf.float32,shape=(None,OUTPUT_NODE))\n",
    "    Yo = forward(Xi,REGULARIZER)\n",
    "\n",
    "\n",
    "    # loss_mse = tf.reduce_mean(tf.square(Yo-Yi))\n",
    "    loss_mse = tf.reduce_mean(tf.square(tf.cast(tf.equal(tf.argmax(Yo, 1), tf.argmax(Yi, 1)),tf.float32)-1.0))\n",
    "    loss_cem = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=Yo,labels=tf.argmax(Yi,1)))\n",
    "    lossA = loss_mse\n",
    "    lossB = loss_cem\n",
    "    lossC = loss_mse + tf.add_n(tf.get_collection('losses'))\n",
    "    lossD = loss_cem + tf.add_n(tf.get_collection('losses'))\n",
    "    loss  = lossD\n",
    "\n",
    "\n",
    "\n",
    "    learning_rate_exponential_decay = tf.train.exponential_decay(\n",
    "                                        LEARNING_RATE_BASE,\n",
    "                                        global_step,\n",
    "                                        mnist.train.num_examples/BATCH_SIZE,\n",
    "                                        LEARNING_RATE_DECAY_RATE,\n",
    "                                        staircase=True)\n",
    "    learning_rateA = learning_rate_exponential_decay\n",
    "    learning_rateB = 0.01\n",
    "    learning_rate  = learning_rateA\n",
    "\n",
    "\n",
    "\n",
    "    train_step_GradientDescentOptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    MOMENTUM=0.9\n",
    "    train_step_MomentumOptimizer = tf.train.MomentumOptimizer(learning_rate,MOMENTUM).minimize(loss,global_step=global_step)\n",
    "    train_step_AdamOptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "    train_stepA = train_step_GradientDescentOptimizer\n",
    "    train_stepB = train_step_MomentumOptimizer\n",
    "    train_stepC = train_step_AdamOptimizer\n",
    "\n",
    "    train_step  = train_stepA\n",
    "\n",
    "\n",
    "    #滑动平均\n",
    "\n",
    "    if USE_EMA__ :\n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "        ema_op = ema.apply(tf.trainable_variables())\n",
    "        with tf.control_dependencies([train_step,ema_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "    else:\n",
    "        train_op = train_step\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #实例化saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #测试准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Yo, 1), tf.argmax(Yi, 1)),tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        #初始化\n",
    "        init=tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # #断点续训\n",
    "        # ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        # if ckpt and ckpt.model_checkpoint_path:\n",
    "        #     saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "\n",
    "        Xt,Yt = (mnist.test.images,mnist.test.labels)\n",
    "        for i in range(STEPS):\n",
    "            #训练模型\n",
    "            Xs,Ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={Xi:Xs,Yi:Ys})\n",
    "\n",
    "            if  (i > 0  and  i % 500 == 0) or (i == 1) :\n",
    "                #插入测试\n",
    "                loss_v = sess.run(loss,feed_dict={Xi:Xt,Yi:Yt})\n",
    "                acc_v  = sess.run(accuracy,feed_dict={Xi:Xt,Yi:Yt})\n",
    "                print(\"1\\t After %5d steps, loss on train batch  is \\t %f\"%(i,loss_value))\n",
    "                print(\"2\\t                  loss on test  batch  is \\t \\t %f\"%(loss_v))\n",
    "                print(\"3\\t                  acc  on test  batch  is \\t \\t \\t %f\"%(acc_v))\n",
    "\n",
    "            if i % 2000 == 0 :\n",
    "                #保存模型\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_SAVE_NAME),global_step=global_step)\n",
    "\n",
    "\n",
    "def test(mnist_test):\n",
    "    print(\"test\")\n",
    "    with tf.Graph().as_default() as g:\n",
    "        Xt= tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "        Yt= tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "        Yo = forward(Xt,REGULARIZER)\n",
    "\n",
    "        if USE_EMA__ :\n",
    "            ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "            ema_restore = ema.variables_to_restore()\n",
    "            saver = tf.train.Saver(ema_restore)\n",
    "        else:\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Yo, 1), tf.argmax(Yt, 1)),tf.float32))\n",
    "\n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/'[-1]).split('-')[-1]\n",
    "                    acc=sess.run(accuracy,feed_dict={Xt:mnist_test.test.images,Yt:mnist_test.test.labels})\n",
    "                    print(\"1\\t After %5d steps, test acc is \\t %f\"%(global_step,acc))\n",
    "                else:\n",
    "                    print('No ckpt file found')\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS)\n",
    "\n",
    "def pre_pic(picName):\n",
    "    print(\"pre_pic\")\n",
    "    img  = Image.open(picName)\n",
    "    reIm = img.resize((28,28),Image.ANTIALIAS)\n",
    "    im_arr = np.array(reIm.convert('L'))\n",
    "    threshold = 50\n",
    "\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            im_arr[i][j] = 255 - im_arr[i][j]\n",
    "            if (im_arr[i][j] < threshold):\n",
    "                im_arr[i][j] = 0\n",
    "            else:\n",
    "                im_arr[i][j] = 255\n",
    "\n",
    "    nm_arr    = im_arr.reshape([1,784])\n",
    "    nm_arr    = nm_arr.astype(np.float32)\n",
    "    ima_ready = np.multiply(nm_arr,1.0/255.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def application():\n",
    "\n",
    "    print(\"application\")\n",
    "\n",
    "    if 0:\n",
    "        testNum = input(\"input the number of the test pictures:\")\n",
    "        for i in range(testNum):\n",
    "            testPic = raw_input(\"the path of the pciture:\")\n",
    "            testPicArr = pre_pic(testPic)\n",
    "            preValue = restore_model(testPicArr)\n",
    "            print(\"The prediction number is:\",preValue)\n",
    "    else:\n",
    "        testPic='./test/5.png'\n",
    "        testPicArr = pre_pic(testPic)\n",
    "        preValue = restore_model(testPicArr)\n",
    "        print(\"The prediction number is:\",preValue)\n",
    "\n",
    "\n",
    "\n",
    "def restore_model(testPicArr):\n",
    "    with tf.Graph().as_default() as tg:\n",
    "        Xp = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "        Yp = forward(Xp,REGULARIZER)\n",
    "        preValue = tf.argmax(Yp,1)\n",
    "\n",
    "\n",
    "        if USE_EMA__ :\n",
    "            ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "            ema_restore = ema.variables_to_restore()\n",
    "            saver = tf.train.Saver(ema_restore)\n",
    "        else:\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                preValue = sess.run(preValue,feed_dict={x:testPicArr})\n",
    "                return preValue\n",
    "            else:\n",
    "                print('No ckpt file found')\n",
    "                return -1\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    mnist = input_data.read_data_sets('./mnist_data/',one_hot=True)\n",
    "\n",
    "    # # 返回 训练 验证 测试集 子集样本数\n",
    "    # print(\"train data size:\",mnist.train.num_examples)\n",
    "    # print(\"validation data size:\",mnist.validation.num_examples)\n",
    "    # print(\"test data size:\",mnist.test.num_examples)\n",
    "\n",
    "    # #返回数据和标签\n",
    "    # print(mnist.train.images[0])\n",
    "    # print(mnist.train.labels[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    USE_EMA__ = False\n",
    "\n",
    "    INPUT_NODE  = 784\n",
    "    OUTPUT_NODE = 10\n",
    "    LAYER1_NODE = 500\n",
    "\n",
    "    BATCH_SIZE  = 200\n",
    "    STEPS       = 4500\n",
    "\n",
    "    REGULARIZER               = 0.0001\n",
    "    LEARNING_RATE_BASE        = 0.1\n",
    "    LEARNING_RATE_DECAY_STEPS = 1000\n",
    "    LEARNING_RATE_DECAY_RATE  = 0.99\n",
    "    MOVING_AVERAGE_DECAY      = 0.99\n",
    "\n",
    "    MODEL_SAVE_PATH = \"./model/\"\n",
    "    MODEL_SAVE_NAME = \"mnist_model\"\n",
    "\n",
    "    MAIN_STEP = 2\n",
    "\n",
    "    if(MAIN_STEP==0):\n",
    "        backward(mnist)\n",
    "    elif(MAIN_STEP==1):\n",
    "        TEST_INTERVAL_SECS = 5\n",
    "        mnist_test = input_data.read_data_sets('./mnist_data/',one_hot=True)\n",
    "        test(mnist_test)\n",
    "    elif(MAIN_STEP==2):\n",
    "        application()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# 留有问题\n",
    "# 1. 断点续训\n",
    "# 2. restore 不成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
